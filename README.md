# ğŸ•¸ï¸ Web Crawler with classifier System

## ğŸ” Overview

This project implements a **multithreaded web crawler** combined with a **deep learning text classification pipeline**. It systematically crawls a specified website, downloads and saves HTML pages, preprocesses text data, and trains a neural network classifier to categorize textual content into predefined categories. It also includes a **file system watcher** to automate processing on new file creation or modification and a **priority queue** to efficiently manage URLs to crawl.

The system leverages Python libraries such as:  
- ğŸš **BeautifulSoup** for HTML parsing  
- ğŸ¤– **Keras/TensorFlow** for deep learning  
- ğŸ“š **NLTK** for natural language processing  
- ğŸ‘ï¸â€ğŸ—¨ï¸ **Watchdog** for filesystem event monitoring  

## âš™ï¸ Key Features

### 1. ğŸ•·ï¸ Web Crawler

- ğŸ§µ **Multithreaded crawling:** Utilizes Python threading to run multiple crawler workers concurrently for efficient crawling.  
- â³ **Politeness and revisit control:** Avoids revisiting URLs within 24 hours to reduce server load.  
- ğŸ”— **URL normalization:** Handles relative URLs and normalizes links to avoid duplicates.  
- âš ï¸ **Error handling:** Captures and logs URL errors during crawling.  
- âš™ï¸ **Configurable base URL and threads:** Easy customization of the starting point and concurrency level.  
- ğŸ’¾ **HTML content saving:** Stores downloaded pages in a structured directory for offline access and processing.  

### 2. ğŸ§  Text Classification

- ğŸ—‚ï¸ **Dataset preparation:** Uses the BBC text classification dataset as an example, with an 80/20 train-test split.  
- ğŸ”¢ **Text tokenization:** Converts raw text to numerical features using Keras tokenizer limited to the top 1000 words.  
- ğŸ·ï¸ **Label encoding:** Transforms categorical labels into one-hot vectors for classification.  
- ğŸ•¸ï¸ **Neural network model:** Implements a simple dense feedforward neural network with dropout for regularization.  
- ğŸ¯ **Training with validation:** Trains the model with early stopping to prevent overfitting.  
- ğŸ“Š **Evaluation:** Reports test accuracy and loss.  
- ğŸ“ˆ **Confusion matrix visualization:** Displays normalized confusion matrix for model performance insight.  
- ğŸ“ **Sample predictions:** Prints examples comparing predicted vs actual categories.  

### 3. ğŸ‘ï¸ Folder Watcher (Watchdog)

- â° **Real-time monitoring:** Watches a target directory for new or modified files.  
- ğŸ”„ **Automatic preprocessing:** Triggers text preprocessing and classification upon detecting file changes.  
- ğŸ—‚ï¸ **Event handling:** Differentiates between file creation and modification events.  

### 4. ğŸ“¥ Priority Queue for URL Management

- ğŸ›ï¸ **Custom priority queue:** Ensures URLs are processed in order of priority with FIFO for same priority items.  
- âš–ï¸ **Prioritization based on content type:** Supports assigning crawling priority based on predefined categories.  

### 5. ğŸ§¹ Text Preprocessing Utilities

- âœ‚ï¸ **Tokenization and cleaning:** Removes punctuation, stopwords, and short tokens from text documents.  
- ğŸ“‚ **Directory processing:** Cleans all documents in a directory, supporting separate train/test splits.  
- ğŸ’¾ **Dataset saving:** Provides functionality to save processed datasets for downstream use.  

---

## ğŸš€ Usage

1. **Crawler Configuration**  
   Modify the `conf` dictionary to set the base URL and number of crawling threads.

2. **Prepare Directory**  
   The crawler automatically cleans and creates the output directory for storing crawled pages.

3. **Run Crawler Threads**  
   Initialize and start multiple instances of the `Crawler` thread to begin crawling and saving web pages.

4. **Train Classifier**  
   Use the prepared dataset (`bbc-text.csv`) to train the neural network classifier. Modify hyperparameters as needed.

5. **Monitor Folder**  
   Use the `Watcher` class to monitor new or updated files and trigger processing.

6. **Analyze Results**  
   Visualize confusion matrices and inspect sample predictions to evaluate classification performance.

---

## ğŸ“‹ Requirements

- Python 3.7+  
- Libraries:  
  - `numpy`  
  - `pandas`  
  - `keras` / `tensorflow`  
  - `nltk`  
  - `beautifulsoup4`  
  - `watchdog`  
  - `matplotlib`  
  - `scikit-learn`  
- Download NLTK stopwords before running:  
  ```python
  import nltk
  nltk.download('stopwords')
